<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-01-20T16:40:02-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Prab’s Code Blog</title><subtitle>My blog about programming and software development.</subtitle><author><name>Prabhakar Kumar</name></author><entry><title type="html">Setup Arch Linux on a Dell XPS 13 with Windows 10 dual boot</title><link href="http://localhost:4000/2020/01/15/setup-arch-linux-on-dell-xps-13-7390.html" rel="alternate" type="text/html" title="Setup Arch Linux on a Dell XPS 13 with Windows 10 dual boot" /><published>2020-01-15T17:48:53-08:00</published><updated>2020-01-15T17:48:53-08:00</updated><id>http://localhost:4000/2020/01/15/setup-arch-linux-on-dell-xps-13-7390</id><content type="html" xml:base="http://localhost:4000/2020/01/15/setup-arch-linux-on-dell-xps-13-7390.html">### Intro

This article details steps for setting up a functional Arch Linux installation (with a beautiful Deepin Desktop Environment GUI) on a Dell XPS 13, along with the pre-installed Windows 10
in a dual boot configuration. As a developer, I needed an ultrabook class laptop which wasn't underpowered. Dell XPS 13 fits perfectly in this segment. It is extremely light, has a
unibody design with sturdy hinge and is specwise pretty juicy. After being spoilt with a Macbook Pro for a few years during my previous employment, I never liked the touchpads of
non-mac laptops. Dell XPS 13's was the first one I did. I believe new HP Spectre and Lenovo carbon laptops might have good touchpads but I haven't played with them yet.

I bought the 2019 version of Dell XPS 13 7390 with the following specs

* Core i7 10th Gen 10710U processor ([CPU Benchmark](https://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i7-10710U+%40+1.10GHz&amp;id=3567))
* 16GB LPDDR3 2133 MHz RAM, 
* 1TB M.2 PCIe NVMe SSD
* 13.3 inch 4K touchscreen display
* Backlit chiclet keyboard with Fingerprint Reader on the power button

It cost me $1705.01 USD inclusive of the 10% Washington state sales tax. Too bad, Dell released the 2020 version a few days after I purchased it but I console myself by thinking that
similar configuration is 500 dollars costlier for the 2020 version as of this writing.

Before you start judging me for why didn't I buy the Developer Edition as it should have flawless driver support for Linux, I would like to clarify that I did
consider it but for the similar configuration it was only 10 dollars cheaper and didn't have a fingerprint sensor; the Dell Website didn't list these in the spec page of
developer edition. I made up my mind to fight driver issues head-on on the Windows edition, should they come up; but touchwood, I didn't get any that hinder my ability to use the laptop effectively!

#### Requirements

These set of steps should take less than an hour of your time, assuming you have a fairly performant internet connection as per 2019-2020 standards accessible via a WPA2 encrypted WiFi network.
This article also assumes that you have some basic understanding and familiarity with setting up and running any Linux distro for a while.
This article doesn't require you to have another laptop with you during the installation process. You should have a USB thumbdrive (a.k.a. pendrive) and the USB-C to USB-A adapter
that comes free with the XPS 13 that may look something like below image.
![Thumb drive](/images/thumb_drive.png)

#### Why Arch

I have been an Arch user since 2011 and never had any issues with it. [ArchWiki](https://wiki.archlinux.org/) has probably the best documentation among all distros, and
[AUR](https://aur.archlinux.org/) has a comprehensive collection of packages that can be installed if not found in official repositories. With a rolling release model so you DO NOT have to
update your distro every 6 months the new version comes, and serves the latest versions of almost all packages from its repositories. Despite of being rolling release,
it is very stable. It is very lean; on top of a basic Linux system it's the user who makes all choices for installing only the packages required. And because of rolling release nature, you have
the goodies rolling in everyday. The package manager `pacman` is very fast and processes installation of packages in a jiffy.
Using Arch gives you a feeling of being in control of everything.

### Step 1: Download the arch installation image and prepare the Arch Linux boot disk

Boot into the pre-installed Windows 10 of your Dell XPS 13 and download the Arch Linux latest installation image from this page: [Arch Linux - Downloads](https://www.archlinux.org/download/).
I chose to download the image over BitTorrent protocol. I find it usually faster than HTTP, and it also frees up bandwidth for other needy users to download from HTTP server.

Once you have the installation image downloaded, you can create a boot disk with it. I use the `DD for windows` tool for this which can be downloaded from this
[download page](http://www.chrysocome.net/download). The page appears confusing on first looks as it has lots of files listed for download. You can download the file named
`dd-0.6beta3.zip` and extract it, then open up a command window or powershell window, and navigate to the extracted folder.

The following command will create a Arch boot image in a pendrive that has been inserted and has been assigned the drive letter D. Adjust the command to reflect the correct path
of the downloaded Arch image and the drive letter assigned to the thumbdrive. Ensure that you put the backslashes exactly as shown in the `of` parameter.

```bat
.\dd if=C:\Users\your-windows-username\Downloads\archlinux-2020.01.01-x86_64.iso of=\\.\d: bs=16M
```
### Step 2: Disable Bitlocker and partition the disk for Arch

Microsoft's Bitlocker disk encryption can annoy you by forcing you to enter a long encryption key when you attempt to boot Windows after Arch installation. So it is a good idea to disable Bitlocker
beforehand. Type `Manage BitLocker` in start menu and disable it for C drive. Or you can go to `Control Panel` &gt; `System and Security` &gt; `Bitlocker drive encryption` and disable it from there.

Now you need to free some space up for your Linux installation. You can do this by using the Disk Management tool that comes built-in with Windows 10 or the partitioning tools for linux during
installation process. In this article we are doing it from Windows. Microsoft has nice online documentation for it here:
[Disk Management](https://docs.microsoft.com/en-us/windows-server/storage/disk-management/overview-of-disk-management)

Press the start button, or the Window key on the keyboard and start typing `Create and Format Disk Partitions`. Launch the program. It will show you the layout of the hard disk partition
structure. By default, it comes with a 500MB EFI partition, a very large C drive partition, a WINRETOOLS parition, a factory image partition and a DELLSUPPORT parition. You can shrink the
existing C drive parition to make space for Linux.

Right click on the C drive partition and choose `Shrink Volume ...`. Enter the size for the new partition. The row under it should autocalculate and show you the final size of C drive.
Make sure to keep it reasonable for your Windows needs. I entered 500000 and it resulted in a 488GB of raw unallocated free space.

I chose to create a 8GB volume for swap space and remaining space for the `/` partition. For this, just right click the newly created unallocated space and click `New Simple Volume`
and enter 8192 MB as the size. Choose to not assign a drive letter when asked. Then right click the remaining space, click `New Simple Volume` and allocate all the remaining space for  the volume
for `/` partition. 

Some people create separate partition for `/home` as well which in my opinion is actually a good idea, because if for some reason your Linux installation gets borked, you can nuke the `/` partition,
install a fresh copy of your OS on it (even different distro altogether) and retain everything in the `/home` by setting the mount point in `fstab`. This makes everything look like it used to
earlier (assuming certain conditions hold), because of all the user level customizations in the desktop environment being retained in users' home directory. I find separate partitions for /usr and
/var on personal computers an overkill. For the sake of simplicity we are not creating a separate `/home` partition in this article.

Ultimate result should look something like the below image. If you have created more partitions, they will appear too.

![Disk Layout](/images/disk_layout.jpg)

### Step 3: Change SATA mode to AHCI and disable SecureBoot

Time to fiddle with some BIOS settings. Before we start installation, we need to change the SATA operation mode in BIOS to AHCI so that the Arch boot disk can recognize the hard disk volumes. If you do not
change the SATA mode to AHCI, Arch installation disk won't recognize your NVMe M.2 hard disk in XPS 13.

Get into the BIOS settings by rebooting the computer and pressing F12 repeatedly until you see the text `Preparing one time boot menu` in the top right corner of the screen.
Then click `BIOS Setup1 and change the SATA operation to AHCI under `System Configuration` section of the left sidebar. Here's a screenshot that may come handy identifying where to do this.

![SATA Mode AHCI](/images/sata_mode.png)

Also, disable SecureBoot in BIOS because it won't allow setting up Grub which is the de-facto bootloader for Linux distributions. Below screenshot shows how to disable SecureBoot.

![Disable secure boot](/images/secure_boot.jpg)

Save the change which will result in a system restart.

### Step 4: Boot the arch installation media

Press the F12 key repeatedly while the ssytem is starting and get into the One Time Boot Menu again and choose the pendrive as the boot device, as shown in the screenshot below.
![Boot device](/images/boot_device.png)

If the disk image was created correctly you should see the Arch Boot Menu as shown below. Choose the first option and press enter.
![Arch boot menu](/images/arch_boot.png)

You should see the `root@archiso` prompt in a few moments. Please note that, on the 4K display XPS 13, the letters in the terminal during installation look extremely small. However, you have to
face this problem only during installation and after the installation of a GUI everything should start looking good.

### Step 5: Connect to the network and enable NTP time sync

I assume you have a WiFi network in range. You can use the `wifi-menu` command that comes pre-installed in the installation disk. It has a nice curses based UI where you can choose the WiFi network
from a menu of options and enter its password to connect to it. Test the connection by running `ping archlinux.org`.

Now set the time with NTP by running

```sh
timedatectl set-ntp true
```

### Step 6: Setup filesystems

Run the below command

```sh
ls /dev/nvme*
```
to check for the block devices. It is highly likely that you should be able to identify `/dev/nvme0n1` as the block device representing your hard disk, which is because all XPS 13s come
with NVME based M.2 SSDs. You can verify this by running `fdisk -l /dev/nvme0n1` which should output the list of Windows partition that already exist and the raw partitions.

Carefully check the output of the fdisk command above, and figure out the partition number for the swap and / partitions that we created in Step 1. For me, the output looks like this.

```text
fdisk -l /dev/nvme0n1

Disk /dev/nvme0n1: 953.89 GiB, 1024209543168 bytes, 2000409264 sectors
Disk model: PC601 NVMe SK hynix 1TB                 
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: 7EED0FB4-482A-45A9-A7CE-55E525352534

Device              Start        End    Sectors   Size Type
/dev/nvme0n1p1       2048    1394687    1392640   680M EFI System
/dev/nvme0n1p2    1394688    1656831     262144   128M Microsoft reserved
/dev/nvme0n1p3    1656832  944373759  942716928 449.5G Microsoft basic data
/dev/nvme0n1p4  944373760  961150975   16777216     8G Microsoft basic data
/dev/nvme0n1p5  961150976 1968371711 1007220736 480.3G Microsoft basic data
/dev/nvme0n1p6 1968373760 1970401279    2027520   990M Windows recovery environment
/dev/nvme0n1p7 1970401280 1997346815   26945536  12.9G Windows recovery environment
/dev/nvme0n1p8 1997348864 2000408575    3059712   1.5G Windows recovery environment
```

I was able to figure out that `/dev/nvme0n1p4` was the 8GB partition I created for swap and the `/dev/nvme0n1p5` was the partition I created for root filesystem. Enable swap on the swap parititon
and format the partition for root filesystem into ext4.

```sh
mkfs.ext4 /dev/nvme0n1p5
mkswap /dev/nvme0n1p4
swapon /dev/nvme0n1p4
```

Now that we are ready with filesystems, we can go ahead mounting them and starting installation.

### Step 7: Mount partitions and install needed packages

Create a directory `/mnt/root` to mount the root filesystem and EFI system partition on `/mnt/root/boot`.

```sh
mkdir -p /mnt/root /mnt/root/boot
mount /dev/nvme0n1p5 /mnt/root
mount /dev/nvme0n1p1 /mnt/root/boot
```

Open the file `/etc/pacman.d/mirrorlist` in a text editor like `vim` or `nano`. Bring a few mirror URLs closest to you geographically to the top of the file. Since, I live on the west coast of USA,
I brought a few US west coast pacman mirrors onto the top. This should ensure that the package installation command runs fast.

Now install the necessary packages using pacstrap.

```sh
pacstrap /mnt/root base linux linux-firmware base-devel grub os-prober intel-ucode alsa deepin deepin-extra sudo lightdm lightdm-deepin-greeter
```

I like deepin as my desktop environment as it looks beautiful and has a perfect traditional desktop look and feel. If you want Gnome, or KDE, or LXDE, or LXQT, or XFCE or any other desktop environment
instead, feel free to modify the above command and install corresponding packages.

### Step 8: Configure base system

Generate an fstab file by running `genfstab -U /mnt/root &gt;&gt; /mnt/root/etc/fstab`.

Now chroot into the system to configure some other necessary stuff. Arch installation image has a handy chroot wrapper script named `arch-chroot`, which chroots into the installed root filesystem
and also binds necessary virtual filesystem mounts like `proc`, `sys` and `dev` correctly from the currently running installation OS.

```sh
arch-chroot /mnt/root
```

#### Set timezone

Set the time zone by creating a symlink to the zoneinfo file for your region at `/etc/localtime`. Command below is for US West Coast. You can change it according to your region.
Also, set the clock to use UTC.

```sh
ln -sf /usr/share/zoneinfo/Americas/Los_Angeles
hwclock --systohc
```

#### Set locale

Open `/etc/locale.gen` in vi or nano and uncomment the `en_US.UTF-8 UTF-8` line. Then run `locale-gen`. After this, open `/etc/locale.conf` and add a line `LANG=en_US.UTF-8`.

#### Network configurations

Create the hostname file `/etc/hostname` and add a name for your laptop, for example `my-xps`. Also update the hostname in `/etc/hosts` so it looks like below:

```text
127.0.0.1   localhost
::1         localhost
127.0.1.1   my-xps.localdomain my-xps
```

Deepin comes with NetworkManager but it didn't work as expected for me for connecting to WiFi. So you might want to disable NetworkManager and install netctl.

```sh
systemctl disable NetworkManager
pacman -S netctl wpa_supplicant dhcpcd
```

You can also create a `netctl` profile right away so that you do not need to struggle connecting to the internet when you reboot after the installation is finished.

Create a file named `/etc/netctl/MyWiFi` and put the following content. This assumes that your home WiFi network name is `MyWiFi`. I also faced lag and disconnection issues with
5GHz wifi networks so for now I am using a 2.4GHz network only. I haven't spent time figuring out the cause and fixing it, I may update this article when I do.

```text
Description='My home WiFi connection'
Interface=wlp2s0
Connection=wireless

Security=wpa
IP=dhcp

ESSID='MyWiFi'  # &lt;=== Put your WiFi network's name here
Key='12345678' # &lt;== Put your WiFi password here
```

#### Build initramfs

Run `mkinitcpio -P` to build the initramfs.

### Set root password 

Set a root password by running the `passwd` command.

### Step 9: Configure Grub

During pacstrap we installed `grub` and `os-prober` so, now we just need to install grub and create a config for it. If `os-prober` is installed, `grub-mkconfig` command automatically
detects Windows partition, if any and adds an entry for booting into it in the boot menu.

```sh
grub-install --target=x86_64-efi --efi-directory=/mnt/root/boot --bootloader-id=GRUB
grub-mkconfig -o /boot/grub/grub.cfg
```

### Step 10: Create a user and enable sudo

Create a non-root user and add it to necessary groups (at least `wheel`). Replace the username `master` in below command with your desired username.

```sh
useradd -m -G wheel -s /bin/bash master
```

Also set a password for this account by running `passwd master`.

Run `visudo` to open sudoers file. Find the line that allows members of group wheen to run sudo commands without password and uncomment it. It should look something like below:

```text
%wheel ALL=(ALL) NOPASSWD: ALL
```

However, it is not recommended for everybody to configure sudo without password. You might want to configure it to be used with password instead by uncommenting the following line instead.

```text
%wheel ALL=(ALL) ALL
```

### Step 11: Prepare for reboot

Now your arch installation is ready to boot into. But before that, we need to perform one final configuration, which is to configure `lightdm` - the display manager to use `deepin`'s greeter.

Open the file `/etc/lightdm/lightdm.conf`. Find the line that says `#greeter-session`, uncomment it and update it to use deepin's greeter as shown below.

```text
greeter-session=lightdm-deepin-greeter
```
Also, enable lightdm by running `systemctl enable lightm`. Now reboot by typing `reboot`. You should be greeted by a beautiful login screen. When you login, you will see a desktop resembling the below
screenshot.

![Deepin desktop](/images/deepin_desktop.png)

Deepin desktop comes with a dock by default but I prefer a task bar instead. You can right click the dock and change the mode to `Efficient Mode`.

![Deepin taskbar](/images/dock_mode.png)

You might also want to change the size of icons to small instead of large.

![Deepin taskbar size](/images/dock_size.png)

If the font sizes look too small or too large, you can adjust the scaling factor by right clicking the desktop, choosing `Display Settings` as shown in the screenshot below.

![Display Scaling](/images/display_scale.png)

### Step 13: Fix windows

Since we overwrote Windows's EFI partition, it will refuse to boot. But there is a simple trick to fix that. You just need to boot Windows in safe mode once and it figures out stuff automatically.
Kudos to Microsoft for this! If you try booting Windows from the Grub menu it will get into the dreaded blue screen of death. But from there you will get an option to attempt to diagnose
and fix boot problems. There you can choose to boot into safe mode. Once you have successfully made it into safe mode, you should also make a change in Windows registry so that it treats hardware
clock to be in UTC. This way the time shown in Linux as well as Windows will be similar. For that, just open a command prompt with administrator privileges and run the following command:

```bat
reg add &quot;HKEY_LOCAL_MACHINE\System\CurrentControlSet\Control\TimeZoneInformation&quot; /v RealTimeIsUniversal /d 1 /t REG_QWORD /f
```

Or you could open `regedit` and manually navigate to the above key and add a new QWORD value of 1 for `RealTimeIsUniversal`.

### Step 14: All set

Assuming that you sailed through the process smoothly, congratulations! You now have your Dell XPS 13 with a dual boot setup of Windows 10 and Arch Linux. If you are a developer like me, you might want
to change your shell to zsh, install Git, Vim, htop, Ruby, NodeJS, JDK, Postman, Docker, Visual Studio Code, IntelliJ Idea Community Edition and other development tools and platforms to facilitate your
development workflow. As general utilities, you might want to install Gimp for photo editing, imagemagick, VLC media player, xarchiver, p7zip-full, unrar, Evince for PDF viewing, Chromium and Firefox
for web browsing and Audacious for listening to your music collection.

Here's an obligatory screenshot (compressed JPEG) of my final setup.

![Arch Desktop](/images/arch_desktop.jpg)

### Known Issues

I have found the following issues on my Dell XPS 13 (with factory installed Windows) while using Arch Linux:

- Looses connection to 5G WiFi network intermittently (however 2.4GHz networks work perfectly)
- Cannot use NetworkManager to connect to WiFi networks. Netctl works flawlessly, but to make your life easier install wifi-menu from AUR.</content><author><name>Prabhakar Kumar</name></author><category term="archlinux" /><category term="dell-xps-13" /><summary type="html">Intro</summary></entry><entry><title type="html">Run your webapps on Kubernetes for cheap</title><link href="http://localhost:4000/2019/04/11/run-your-webapps-on-kubernetes-for-cheap.html" rel="alternate" type="text/html" title="Run your webapps on Kubernetes for cheap" /><published>2019-04-11T09:48:53-07:00</published><updated>2019-04-11T09:48:53-07:00</updated><id>http://localhost:4000/2019/04/11/run-your-webapps-on-kubernetes-for-cheap</id><content type="html" xml:base="http://localhost:4000/2019/04/11/run-your-webapps-on-kubernetes-for-cheap.html">### TL;DR

This post details how to spin off a 3 node Kubernetes cluster on Google Cloud, paying close to 7 dollars a month for it and host multiple database backed dynamic websites and API apps.

### Disclaimer

This is a hands on article to get started with Kubernetes. Some of the terminology and descriptions in this article have been simplified to make it approachable to people unfamiliar with
Kubernetes. For technical purity, Kubernetes's official documentation can be consulted.

### Intro

Kubernetes is the new cool kid on the block (bye bye blockchain!). Listing it as an skill in your LinkedIn profile will definitely get you some recruiter attention.
It is *the solution* for all your dev-ops woes!

I used to run all of my hobby projects in docker containers with lifecycles managed by docker-compose files. I had a VM from [scaleway](https://scaleway.com) with 4 gigs of memory,
100 gigs of SSD storage and dual core x86 CPU for which I used to pay around 5 Euros a month. I was happy until I learnt about Kubernetes and then I realized *ignorance is bliss*.
After reading this post, you will be able to spin off a 3 node Kubernetes cluster on Google Cloud with 0.6GB memory each and 1 vCPU each and pay roughly ~7 dollars a month for it!
If you want a beefier cluster you can choose a different configuration for nodes and for 40 dollars a month you get 11.25GB of memory at your disposal, three compute cores with which you can do
wonders - host several high traffic webapps backed by different databases! Adding to that, with Google Cloud's free trial program you can have this cluster running for free for almost an year!

My three node cluster currently runs `mongodb`, `postgres`, `nginx`, two rails web applications, a nodeJS web application, a dotnetcore web application, a spring boot based Java REST API application
which powers an Android app, and still has a lot of resources remaining to do 3-4 times more of what it currently does. All the web apps are fronted by nginx and configured with different server blocks
for different domain names (a.k.a virtual hosts). All the domain names have their own letsencrypt https certificates that automatically renew every three months. Two of the apps use mongodb and
the other two postgres.

#### Single VM vs Cluster

Running all your workloads in a single VM is like putting all the eggs in the same basket. If ever the VM is restarted for updating the Kernel, reclaim some leaked memory or something else;
all the apps hosted on the box incur downtime. Running on Kubernetes is better, because:

1. Cloud provider (GCP) manages updates and patches on your VMs, so you don't need to explicitly restart nodes. In case of node restarts, Kubernetes ensures relocating workloads running on the unavailable VM.
2. Kubernetes restarts containers automatically when underlying process crashes due to some issue.
3. Kubernetes handles load balancing of requests and scaling of resources to fulfill the requests. For example, you can tell Kubernetes declaratively to run more instances of your webapp when the load is higher.
4. Kubernetes sets up internal networking in the cluster so that your apps can talk to each other. This enables you to build microservices based large scale services.

Besides, using Kubernetes to run your compute infrastructure is considered cool in 2019!

### What is Kubernetes?

Kubernetes's official website writes:

&gt; Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.

This might not be quite an approachable description for everybody. Here is my attempt:

&gt; Kubernetes is like an operating system, for your distributed computing cluster that lets you run your scalable applications without being bothered about nodes in the cluster going down, request routing, load balancing, networking and other devops hassles.

### Let's get started

#### Step 1: Setup a 3-node cluster on Google Cloud

Register on Google Cloud, if you already haven't. Create a project, setup billing. Click the hamburger menu icon (![menu](/images/hamburger.png)) on the top left, and in *Compute* section,
click *Kubernetes Engine*. Click the `Create Clusters` button. Make sure in the left pane, cluster template is set to `Standard cluster`. In the right pane, name your cluster;
for example *blog-cluster*. Fill in the settings as listed below:

* *Location type*: Zonal
* *Zone*: us-central1-a

Under node pools:

* *Number of nodes*: 3
* *Machine type*: `n1-standard-1` (this will cost roughly $40/month for 3 nodes). Choose `f1-micro` for $7/month instead.

Click *More node pool options* and ensure that the following are set:

* *Enable autoscaling*: off
* *Boot disk type*: Standard persistent disk
* *Boot disk size*: 10 GB
* *Enable pre-emptible nodes*: Yes - you must do this for cost cutting. These nodes get terminated within 24 hours and get replaced with another one and cost around one-fourth of regular nodes.
Since Kubernetes can handle node downtimes, we need not worry about our app going down when GCP replaces a VM.

Click *Availability, networking, security, and additional features* and ensure the following:

* *Enable HTTP load balancing*: No - this is costly! We will use Kubernetes's default balancer
* *Enable Stackdriver Logging service*: No
* *Enable Stackdriver Monitoring service*: No
* *Enable Kubernetes Dashboard*: No

Leave all the other settings to their sane defaults. Changing some of them might cost you extra money. Click the `Create` button and wait for the cluster to come up. The clusters page should
show your newly created cluster.

![Clusters](/images/kubernetesclusters.png)

#### Step 2: Connect to your newly minted cluster

Install `kubectl` command line utility for your OS. In the *Clusters* page, click *Connect* and it will show you a `gcloud` command which downloads the necessary authentication information
for the `kubectl` command. Run the command. Once you have done that, test your connection by running `kubectl get nodes` which should return you a list of three nodes.

```bash
% kubectl get nodes
NAME                                          STATUS   ROLES    AGE   VERSION
gke-blog-cluster-default-pool-edcca0b2-4mf2   Ready    &lt;none&gt;   1m    v1.11.7-gke.12
gke-blog-cluster-default-pool-edcca0b2-hd1p   Ready    &lt;none&gt;   1m    v1.11.7-gke.12
gke-blog-cluster-default-pool-edcca0b2-z6f1   Ready    &lt;none&gt;   1m    v1.11.7-gke.12
```

#### Step 3: Prepare your first application

Let's prepare a rails application that uses a `postgresql` database to deploy on this cluster. We will name it `RailsPortal`, you are free to name whatever you want.

```bash
gem install bundler
gem install rails
rails new RailsPortal
bundle exec rails s
```

Last command should start a local server at port 3000 and you should be able to access it at `localhost:3000`. Stop the server by running `Ctrl-C`. Open `config/database.yml` and around the
end change the production db connection settings so that they look like this:

```bash
production:
  &lt;&lt;: *default
  adapter: postgresql
  url: &lt;%= ENV['DATABASE_URL'] %&gt;
```

Also, edit the `Gemfile` and add a line `gem 'pg'` then run `bundle update`. The above changes will configure the production instance of app to use postgresql instead of default sqlite and 
will let us pass the `postgresql` connection string through an environment variable that Kubernetes will setup for us later.

Let's create a scaffold with two fields and see whether things are working fine.

```bash
bundle exec rails g scaffold User name:string about:string
bin/rails db:migrate RAILS_ENV=development
```

This will generate necessary views and controllers which we can use to insert some data and ensure that things are working fine. Second command will commit pending database migrations.
Start the server again  by running `bundle exec rails s` and access the url `http://localhost:3000/users` and click `New User` and try adding a new record.

Now that our test application is ready, let's containerize it. If you do not have docker installed, now is the time to install it and start the docker service.
Once we build the container, we want to push it to a private docker repository. [Docker Hub](https://hub.docker.com) lets you store container images for free. Register on their website then connect
your local `docker` command to your docker hub account by running the following command:

```bash
docker login --username=DockerHubUsername --email=DockerHubEmail
```

For building the container image, we need to create a file named `Dockerfile` which tells docker how to build the container. Create this *Dockerfile* in rails portal's root directory.

```docker
FROM ruby:2.5-alpine

RUN apk add --no-cache --update nodejs postgresql-dev libpq tzdata imagemagick
RUN apk add --no-cache --update --virtual build-deps build-base git
RUN cp /usr/share/zoneinfo/UTC /etc/localtime

COPY . /app

WORKDIR /app
RUN echo 'gem: --no-document' &gt; /etc/gemrc
RUN bundle install --jobs 4 --without development test

RUN apk del build-deps
```

Make sure the ruby version in Dockerfile is consistent with the one in Gemfile. Now build and push the container to docker hub by running following commands:

```bash
docker build -t DockerHubUsername/RailsPortal .
docker push DockerHubUsername/RailsPortal
```

#### Step 4. Architect your infrastructure

![Rails on Kubernetes](/images/kube_rails.png)

We are going to use a simple architecture *client -&gt; Webserver (nginx) -&gt; Web app (rails) -&gt; Database (postgres)* which is quite popular for single VM setups too. Only difference is that instead
of processes running on a VM, these will be containers and will be run on different nodes of the cluster by Kubernetes.

#### Step 5. Plan Kubernetes resources

Kubernetes has the concept of resources. The ones we are going to use in this project are:

+ Deployment
+ Service
+ StorageClass
+ PersitentVolumeClaim
+ PersistentVolume
+ Secret
+ ConfigMap

A *deployment* describes a container to run along with the volumes it needs, environment variables, ports it should listen to, number of replicas of it Kubernetes should run in the cluster etc.
There are many more things we can configure for a deployment, but these are the ones we are going to use in our little project. In this article, I might use the terms deployment and container
interchangeably - because in practice a Kubernetes deployment is a container running in the cluster.

A *service* fronts a deployment by giving it a stable private or public endpoint on which the deployment (running container) can be accessed. Private endpoints are exposed by service of type
ClusterIP and public endpoints are exposed by service of type LoadBalancer. Private endpoints are accessible only inside the cluster, from other containers. For example, a postgres deployment
should only be accessible from deployments running in the cluster, for example a web application.

A *storage class* is to define the characteristics of volumes that can be created on demand. We create this once and create persistent volume claims that refer to it.

A *persistent volume* is a storage accessible from any of the running containers on the cluster which are configured to access it. As containers are stateless, we need a reliable persistent store
to store our stateful data, for example postgres's data files.

A *persistent volume claim* is an access mechanism for a container to access a persistent volume. We will talk about it ahead in the article when we need it.

A *secret* is used to store authentication credentials and expose it to only the resources that need it.

A *config map* is a keyvalue pair, that can be used to store configuration information in the cluster which can be used from a container.

Kubernetes provides us a declarative way of telling it about the resources we need to create (a *goal state* of our infrastructure) and then it tries to create those resources (or achieve the
*goal state*). We declare the properties for resources in *yaml* files and Kubernetes takes care of it.

For our little project, we will be creating the following resources:

+ A deployment for nginx, which we will use to front our web applications (rails)
+ A service of type load balancer for nginx, such that users can access our applications from external the internet
+ A config map for the nginx configuration files
+ A secret to store authentication information for docker hub so that kubernetes can pull the rails application container that we pushed to our private docker hub repository
+ A deployment for rails portal, whose container we built in step 4
+ A service for rails portal, so that nginx can access it forward requests to it
+ A deployment for postgres, which will run the container
+ A service for postgres so that rails application can connect to it
+ A storage class for postgres that defines the kind of storage to be used for persistent volumes. E.g, SSD/magnetic etc.
+ A persistent volume claim for postgres so that the postgres deployment can access the persistent volume
+ A persistent volume for postgres, so that we can have postgres store its data files there

#### Step 6. Setting up nginx

Let's begin by creating a deployment for nginx. Create a directory named nginx. Below gist is for a yaml file that we will use to describe nginx deployment on Kubernetes. Copy
its contents to a file named `nginx-dep.yml` on your local computer.

{% gist 6e870acd7e91273a747996961a2a04c2 %}

It declares that we want to create a resource of type `Deployment`, label it with name `nginx`, use the container `nginx` with tag `mainline-alpine`, and expose the container's port 80.
By default the container image is pulled from docker hub. Now run `kubectl apply -f nginx-dep.yml` to let Kubernetes do its magic.

```bash
% kubectl apply -f nginx-dep.yml
deployment.apps/nginx-dep created
% kubectl get pods
NAME                         READY   STATUS    RESTARTS   AGE
nginx-dep-776c646dc6-g4w9x   1/1     Running   0          25s
```

The output of last command tells us that an nginx deployment is running since 25 seconds. Yes, you just noticed that we used a new term `pods` in the command. A pod can be considered as an application
for all practical purposes. You can find about it more in official Kubernetes documentation.

Let's create an LB now so that we can access the running container from outside. Copy the below contents to a file named `nginx-svc.yml`.

{% gist d76f720c5b9d666965e959861e5e33b3 %}

This file declares that we want a resource of kind `Service` of type `LoadBalancer` which should accept connections on port 80 and direct to port 80 of a service. Which service? The service is selected
using the selector parameter which says to match it with the `app: nginx` which we had specified in our `nginx-dep.yml` file. Let's deploy this file too.

```bash
% kubectl apply -f nginx-svc.yml
service/nginx-lb created
% kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.59.240.1     &lt;none&gt;        443/TCP        4m
nginx-lb     LoadBalancer   10.59.240.108   &lt;pending&gt;     80:31463/TCP   24s
```

This shows that the resource with name nginx-lb and type LoadBalancer has been created. But it is pending an external ip. Wait for some time and run the command again to get the external ip.

```bash
% kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE
kubernetes   ClusterIP      10.59.240.1     &lt;none&gt;         443/TCP        5m
nginx-lb     LoadBalancer   10.59.240.108   34.66.92.238   80:31463/TCP   1m
```

Now, if you try typing the shown external ip in a browser, you should be able to get the nginx's welcome page! Awesome! You can change a domain's A record to point to this IP address and this
will be accessible from any browser through the domain name.

#### Step 7. Setting up postgres

Our webapp will need Postgres to store its data. Since we need postgres's data to be persistent across container and node restarts, we need to create a persistent volume. For that we first
create a storage class with type SSD. Then we create a persistent volume claim that uses this newly created storage class. A persistent volume gets automatically created, when we try to create
a persistent volume claim using the storage class of `kubernetes.io/gce-pd`. We can also club two related storage related declarations into a single yaml file as shown below.

{% gist 45ddec715c5e9840e715db5d1b0282ab %}

Let's create the resources from this yml file and check their statuses.

```bash
% kubectl apply -f postgres-storage.yml
storageclass.storage.k8s.io/postgres-ssd created
persistentvolumeclaim/postgres-disk-claim created
% kubectl get pvc
NAME                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
postgres-disk-claim   Bound    pvc-79f8200b-5c64-11e9-978f-42010a800070   1Gi        RWO            postgres-ssd   12s
% kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                         STORAGECLASS   REASON   AGE
pvc-79f8200b-5c64-11e9-978f-42010a800070   1Gi        RWO            Delete           Bound    default/postgres-disk-claim   postgres-ssd            12s
```

Now that our storage for postgres is all set, we can create the deployment and service. Let's club both the declarations in the same yaml file as we did with the storage. Its a good practice to 
club logically related resources in a single file.

{% gist 62e68cf0f42e24c09e9a91e6cf393b08 %}

In the deployment, we specify that we will run the *postgres* container with tag *10-alpine* from docker hub. We also set environment variables for username, password and data directory. You
should note that we specify a volume mount and configure it to use the persistent volume claim we created earlier and we mount that 1GB storage volume at */data* directory in the container.
We also create a service (with type `ClusterIP` being the default) and configure its app selector to postgres and specify that port 5432 on the container be made available for other apps in the cluster.

Let's create the service and deployment and check the status on the cluster.

```bash
% kubectl apply -f postgres-dep.yml
deployment.extensions/postgres created
service/postgres created
% kubectl get pods
NAME                         READY   STATUS    RESTARTS   AGE
nginx-dep-776c646dc6-g4w9x   1/1     Running   0          8m
postgres-6969669b9f-4f42h    1/1     Running   0          54s
% kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE
kubernetes   ClusterIP      10.59.240.1     &lt;none&gt;         443/TCP        15m
nginx-lb     LoadBalancer   10.59.240.108   34.66.92.238   80:31463/TCP   7m
postgres     ClusterIP      10.59.243.25    &lt;none&gt;         5432/TCP       48s
```

Great! We got postgres running and available on our cluster. Let's login to the container and create a database that will be used by the rails portal to store its data. 

```bash
% kubectl get pods
NAME                         READY   STATUS    RESTARTS   AGE
nginx-dep-776c646dc6-g4w9x   1/1     Running   0          17m
postgres-6969669b9f-4f42h    1/1     Running   0          2m
% kubectl exec -it postgres-6969669b9f-4f42h -- /bin/sh
/ # su postgres
/ $ psql -d postgres
postgres=# CREATE DATABASE railsportal;
CREATE DATABASE
```

Through the first command we copied the name of the pod. Then the next command connected to the running container, and mounted its shell (tty) in interactive mode with our terminal and then ran /bin/sh
on the terminal. This directly dropped us into the container's shell, wehere we switched to the user `postgres`, then ran psql and created a database. Logging-in to a running container comes handy
a lot during debugging sessions.

#### Step 8. Deploy the rails application

Since, we pushed our rails application to a private docker repository on docker hub and we do not want the world to be able to access it because it has our intellectual property (;p),
Kubernetes won't be able to pull the image without authentication information. Let's create a secret that stores our docker hub username and password.

```bash
kubectl create secret docker-registry dockeraccess --docker-server=https://index.docker.io/v1/ --docker-username=YourDockerHubUsername --docker-password=YourDockerHubPassword --docker-email=YourDockerHubEmail
```

The deployment yml file looks as follows:

{% gist 0da2905368a607196a02108e47628bee %}

```bash
% kubectl apply -f rails-portal-dep.yml
deployment.extensions/railsportal created
service/railsportal created
% kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
nginx-dep-776c646dc6-g4w9x     1/1     Running   0          1h
postgres-6969669b9f-4f42h      1/1     Running   0          1h
railsportal-7d6d74f865-n6x56   1/1     Running   0          31s
```
If your container status is CrashLoopBackoff because of some changes you made, you may debug by using the command `kubectl describe pod PodName`.

We need to make changes to nginx config so that nginx forwards all the requests to RailsPortal. For that, let's go back and change the config so that we pass a nginx config file
while starting the container. Create a file named `nginx.conf` with the following content:

{% gist 5a1c295e457121234407136c80a23bd2 %}

Create another file named `railsportal.nginx.conf` with the following content:

{% gist 58a6630d678e0ffabfb88c4eece4caaa %}

Now create two config maps for these files in the cluster:

```bash
% kubectl create configmap nginxconfig --from-file=./nginx.conf
configmap/nginxconfig created
% kubectl create configmap railsportalnginxconfig --from-file=./railsportal.nginx.conf
configmap/railsportalnginxconfig created
```

Update the `nginx-dep.yml` and append the config map settings at the end of the file so that it becomes like below:

{% gist 739dfe55fa710d8c1cffd5bc63bb99c5 %}

Now apply the updated configuration for nginx to the cluster:

```bash
% kubectl apply -f nginx-dep.yml
deployment.apps/nginx-dep configured
```

Opening the website with the public IP should show the rails portal. You can check the logs by running `kubectl logs PodName` where PodName should be replaced with the name of the pod for rails
portal that comes up when you run `kubectl get pods`. If things are not working as expected, you may try logging into the container and poke around.

### Home work

You can try deploying another application on the same cluster. A ExpressJS webapp, talking to a mongoDB database would be a good exercise. You may also go ahead and figure out how to enable auto
scaling. And you may try doing this on Azure or AWS and figure which one comes up cheapest.

### Next steps

Now that we have successfully setup our rails application in Kubernetes, in the next article we will do the following:

+ Enable HTTPS access with Letsencrypt certs and setup auto renew
+ Make nginx serve static assets, instead of Rails
+ Setup auto scaling for times of high load</content><author><name>Prabhakar Kumar</name></author><category term="cloud" /><category term="kubernetes" /><category term="docker" /><summary type="html">TL;DR</summary></entry><entry><title type="html">Make your if statements concise</title><link href="http://localhost:4000/2018/03/17/make-your-if-statements-concise.html" rel="alternate" type="text/html" title="Make your if statements concise" /><published>2018-03-17T07:47:27-07:00</published><updated>2018-03-17T07:47:27-07:00</updated><id>http://localhost:4000/2018/03/17/make-your-if-statements-concise</id><content type="html" xml:base="http://localhost:4000/2018/03/17/make-your-if-statements-concise.html">Sometimes you may encounter a situation where you need to do something if two conditions are satisfied or they both are not satisfied; and do something else
if exactly one of them is satisfied. An example would be:

```java
if ((resultFromServer1 == null &amp;&amp; resultFromServer2 == null) || (resultFromServer1 != null &amp;&amp; resultFromServer2 != null)) {
    log.warn(&quot;Both servers are aware or unaware of the operation&quot;);
    doSomething();
} else if ((resultFromServer1 == null &amp;&amp; resultFromServer2 != null) || (resultFromServer1 != null &amp;&amp; resultFromServer2 == null)) {
    log.err(&quot;Only one server is aware of the operation&quot;);
    doSomethingElse();
}
```

These conditions look messy. We can fix them using XOR! XOR returns true if two conditions are different and false if both conditions are same.
So we can write the above complexity as:

```java
if ((resultFromServer1 == null) ^ (resultFromServer2 == null)) {
    log.err(&quot;Only one server is aware of the operation&quot;);
    doSomethingElse();
} else {
    log.warn(&quot;Both servers are aware or unaware of the operation&quot;);
    doSomething();
}
```

The reverse of XOR is EX-NOR. And interesting thing about that is, in case of booleans EX-NOR is equivalent to both conditions being equal. So, it is also
called as equality gate. Hence, we can write:

```java
if ((resultFromServer1 == null) == (resultFromServer2 == null)) {
    log.warn(&quot;Both servers are aware or unaware of the operation&quot;);
    doSomething();
} else {
    log.err(&quot;Only one server is aware of the operation&quot;);
    doSomethingElse();
}
```</content><author><name>Prabhakar Kumar</name></author><category term="programming" /><summary type="html">Sometimes you may encounter a situation where you need to do something if two conditions are satisfied or they both are not satisfied; and do something else if exactly one of them is satisfied. An example would be:</summary></entry><entry><title type="html">Here’s a fluent builder in C#</title><link href="http://localhost:4000/2018/03/17/heres-a-fluent-builder-in-c-number.html" rel="alternate" type="text/html" title="Here's a fluent builder in C#" /><published>2018-03-17T07:31:34-07:00</published><updated>2018-03-17T07:31:34-07:00</updated><id>http://localhost:4000/2018/03/17/heres-a-fluent-builder-in-c-number</id><content type="html" xml:base="http://localhost:4000/2018/03/17/heres-a-fluent-builder-in-c-number.html">A fluent builder makes creating objects more readable than constructors and more concise than setting properties through setters
on an uninitialized object.
Java devs can use Lombok's `@Builder` annotation to get a fluent builder wired into their classes. Unfortunately, in C# world
no such thing exists. So here's how you can roll out your own.

```c#
public class Animal
{
  public string Name { get; set; }

  public string Type { get; set; }


  #region Fluent builder
  public static Animal Builder()
  {
    return new Animal();
  }

  public Animal WithName(string name)
  {
    this.Name = name;
    return this;
  }

  public Animal WithType(string type)
  {
    this.Type = type;
    return this;
  }

  public Animal Build()
  {
    return this;
  }
  #endregion
}
```

And, to construct and object you can simply use:

```c#
Animal a = Animal.Builder().WithName(&quot;Tommy&quot;).WithType(&quot;Dog&quot;).Build();
```

## Update

C# doesn't need this. I figured it out after reading and writing lot of C# code. C# already has a nice intializer which can be used instead
of a fluent builder:

```c#
Animal a = new Animal
{
    Name = &quot;Tommy&quot;,
    Type = &quot;Dog&quot;
};
```</content><author><name>Prabhakar Kumar</name></author><category term="c-sharp" /><category term="design-patterns" /><summary type="html">A fluent builder makes creating objects more readable than constructors and more concise than setting properties through setters on an uninitialized object. Java devs can use Lombok’s @Builder annotation to get a fluent builder wired into their classes. Unfortunately, in C# world no such thing exists. So here’s how you can roll out your own.</summary></entry><entry><title type="html">Boot linux from grub rescue prompt</title><link href="http://localhost:4000/2018/01/14/boot-linux-from-grub-rescue-prompt.html" rel="alternate" type="text/html" title="Boot linux from grub rescue prompt" /><published>2018-01-14T06:50:50-08:00</published><updated>2018-01-14T06:50:50-08:00</updated><id>http://localhost:4000/2018/01/14/boot-linux-from-grub-rescue-prompt</id><content type="html" xml:base="http://localhost:4000/2018/01/14/boot-linux-from-grub-rescue-prompt.html">If you have a dual boot setup of a Linux based OS and Windows 10, and you have setup grub to choose which OS
to boot; you might have experienced the *grub rescue prompt* which comes up after a Windows 10 update screws up
with the boot files.

Panic not, for it's easy to get back to your beloved Linux distro and fix grub. Here are the steps.

* Find out the Linux partition (skip, if you already know)
    - Run `ls` and it should show you a list of partitions like `(hd0,msdos1) (hd0,msdos2) ...` or in the form `(hd0,gpt1), (hd0,gpt2) ...`
    - Run ls with the name of the partition followed by a / to see the files on the partition. Like so `ls (hd0,gpt1)/`. Remember, that forward slash is important. You have to do this for each parition until you find your linux partition - i.e. you see the list of files named `dev, proc, usr, etc, mnt` etc.
* Set the grub modules prefix
    - Run this `set prefix=(hd0,msdos3)/boot/grub`. This assumes, your Linux partition in previous step was msdos3.
* Set the root partition
    - You can set the linux partition as your root partition by running `set root=(hd0,msdos3)`
* Load the needed modules
    - We need to load the linux module to be able to boot Linux. Run `insmod linux` to do that.
* Find where your kernel and initramfs are located
    - You can run `ls /boot/` and it should show a file named `vmlinuz-linux`. That's your Kernel. At least that's how it is named in Arch Linux and related distros like Antergos and Manjaro. Kernel and Ramdisk have different names on different distros. Centos, Fedora and RHEL have names similar to `vmlinuz-3.10.0-693.11.1.el7.x86_64` for the Kernel. I assume you are smart enough to figure the naming out.
    - You can similarly find your initramfs image. Usually it is named something like `initramfs-linux.img`.
    - Caveats to look out for when finding the Kernel and initramfs image files - Number one, if the file names are versioned, choose the latest ones. Number two, choose the same version for Kernel and initramfs. And the last and number three, don't choose the names that contain `rescue` or `memtest`. They are not the ones we are interested in.
* Boot Linux!
    - Having found out the Kernel and Ramdisk images, let's load them up. Run `linux /boot/vmlinuz-linux root=/dev/sda3 fb rw quiet`. sda3 can probably be hda3 if you have an older machine. And the digit at the end should be same as the root partition number we found in the first step i.e. msdos3 in my case. Now load the Ramdisk by running `initrd initramfs-linux.img`.
    - Having loaded 'em both, let's boot. Run `boot` and bingo, your system should now boot normally!
* Fix grub so that you don't have to go through this ordeal next time
    - Running `sudo grub-install /dev/sda` or in some cases hda instead of sda, should reinstall grub correctly and fix your booting problems.</content><author><name>Prabhakar Kumar</name></author><category term="linux" /><summary type="html">If you have a dual boot setup of a Linux based OS and Windows 10, and you have setup grub to choose which OS to boot; you might have experienced the grub rescue prompt which comes up after a Windows 10 update screws up with the boot files.</summary></entry><entry><title type="html">Ship a REST API with Node, ES6 and MongoDB - Part 1</title><link href="http://localhost:4000/2017/03/22/bootstrap-a-restful-api-app-with-node-es6-mongo-linux.html" rel="alternate" type="text/html" title="Ship a REST API with Node, ES6 and MongoDB - Part 1" /><published>2017-03-22T09:22:40-07:00</published><updated>2017-03-22T09:22:40-07:00</updated><id>http://localhost:4000/2017/03/22/bootstrap-a-restful-api-app-with-node-es6-mongo-linux</id><content type="html" xml:base="http://localhost:4000/2017/03/22/bootstrap-a-restful-api-app-with-node-es6-mongo-linux.html">## Introduction
NodeJS has been gaining a lot of traction recently, especially for web applications. Lots of new web applications
are now being built with tools from  the Node ecosystem. A possible reason is the umpteen number of javascript
developers out in the wild. Other one is  the blazing speed of
[V8 engine](https://en.wikipedia.org/wiki/V8_(JavaScript_engine)). Heck, even shiny desktop apps are now being built
with Node viz. Popcorn Time.

In this post I will walk through everything I did to ship an API app. Ship as in - ready to serve production traffic.

### About the stack
We will be using [Express](https://expressjs.com/) as our web app framework. Express is minimalistic, fast and quietly
gets out of our way when you want to detour through the dirt road. It is unlike convention over configuration style in
[Rails](http://rubyonrails.org/) and [Play framework](https://playframework.com/) where you are good as long as you
follow the conventions recommended by the framework. Going gets tough, if you start deviating. There are frameworks
built specifically for facilitating API building. An example would be [Restify](http://restify.com/). The reason of
not choosing such a framework for this exercise is to understand the intricacies of building an API. Restify would
shield us from understanding lots of stuff.

Also, we wil be writing our javascript in [ES6](http://es6-features.org/) which is cleaner and hands down a better
language than the javascript specced in ES5. Latest version of node still doesn't fully support ES6, so we will use a
transpiler named [Babel](http://babeljs.io/) which transpiles code written by us into code conforming to ES5 for which
node has full support. In the future when node catches up, we can turn off transpilation and run our ES6 code directly
with node.

[MongoDB](https://www.mongodb.com/) will be our database. It is a document oriented NoSQL database. What that means
is - it is unlike the RDBMSes like MySQL or Oracle which let you model the world by defining a schema in form
of tables recording their properties and then linking them with primary and foreign keys. With MongoDB we model our
worldly entities in form of documents which are similar to JSON objects . The biggest advantage of NoSQL databases like
MongoDB is that they are designed to be horizontally scalable - whenever we need to increase performance, we just
add few more machines to the DB cluster and the database engine takes care of redistributing data and query traffic
across all available machines. RDBMSes are limited by theory, in terms of horizontal scalability. A discussion of this
is beyond the scope of this article, but if you are interested in knowing more about it you can browse the interwebs
for this nuisance called CAP theorem.

### About the project
We are going to build an API which can power a web application and/or a mobile app. The app is a social code snippet
sharing system where the users can share frequently encountered code snippets that are used in a developer's day to
day life for solving mundane problems which they are too lazy to code themselves. Usually, as developers we google for
our problem, find link to a stackoverflow question and copy the snippet from there and use from. In our case we let
the users share the snippet, enter a description, select the language, optionally add some tags and publish it. A user
can also thumb up a snippet or mark it as junk. We also allow some experienced users to edit the snippets and/or
descriptions for clarity. We define experience with a formula derived from number of upvotes to their questions by
other users, their age on the site and their time spent on the site. Also, we do away with logins through passwords. 
We provide OAuth based logins through Google, Facebook, Twitter etc. Apart from these functional features, we will
also do some non-functional stuff like generate an SEO friendly URL for each snippet so that Googlebot can find our
snippets and list in search results. Let's wear the product manager hat for a moment and write down the high level
product requirements for our application.

#### As a guest user I should be able to

* search for snipppets through description and/or code matches
* list snippets by language, and sort them by
  - most voted
  - most viewed
* login using Google, Facebook or Twitter auth

#### As a logged-in user I should be able to

* perform snippet operations
  - post a snippet
  - edit my posted snippets
  - view a snippet and see all previous versions of a snippet, if they have been edited
* perform profile operations
  - view my profile
  - refresh upstream information in the profile
  - view others' profiles
  - view activity of a user, like snippets posted, snippets upvoted, snippets edited etc.
* perform friendship operations
  - send friend requests to people
  - accept/reject others' friend requests
  - ignore/block a user
* perform feed operations
  - view a feed which shows my friends' activity on the website
  - like and comment on activities in the feed
* perform leaderboard operations
  - view leaderboard of snippets
  - view leaderboard of users
  - filter leaderboard by language, user location etc.
* perform messaging operations
  - send private messages to my friends
  - send private messages to non-friends
  - view my inbox, categorized with messages from friends/non-friends

### Why API?
Let's take a step back and understand why should we write an API? Why not a simple MVC web application? APIs are great
for scaling, technically as well as logistically. With an API, we define a set of operations, which cover all the
interactions of users and other systems with our application. Then we write client apps, which talk to the API and get
the job done. So, next time you swipe left in gmail to archive an email, you should know that an API call has been
made by the GMail app residing on your phone, to the GMail servers along with some parameters which did the actual
archiving of the email.

What is logistic stability? Ok, I just made up this term! With an API as contract in place, the client app can
independently scale with the actual functional implementation. We can have different people (or teams) working on the
different client apps. As long as they have the API spec they can work independently. Also, the same API can power a
native iOS app, an Android app, a mobile website and a web application. These client apps make API calls to fetch and
display relevant data and perform operations upon user interactions.

In terms of technical scalability, the people working on the backend can independently scale the most used operations
in the API. For instance, if we are getting millions of calls for our compute intensive `findBugs` operation in a
day, we can intelligently increase the resources it needs. One way would be to have a dedicated independent machine of
high configuration just for this operation in the API. And we leverage the power of distributed systems and horizontal
scalability! We will get into the details of this when we talk about shipping once we are code complete.

## Dev Env setup
Before we design our data model, let's get our hands dirty a bit, write a hello world app with Express and get MongoDB
setup on our workstation.

I am an [Arch Linux](https://www.archlinux.org/) devotee. The instructions here should work perfectly on Arch. The
onus of changing the commands to suit your package manager(apt, yum, emerge, brew) is upto you.

### Install node, npm and mongodb
The following commands will install `node`, `npm` and `mongodb` for us. `node` is the javascript runtime executable
which is actually going to run our code. `npm` is the package manager which takes care of resolving dependencies on 
third party libraries, downloading them and setting up their paths correctly so that we have a lot of pre-written
code in form of modules available at our disposal.

Mongodb will also be started and enabled for autostart. If you don't want it to be auto started everytime you boot,
don't execute the last command.

```sh
sudo pacman -Syu node npm mongodb

sudo systemctl start mongodb

sudo systemctl enable mongodb
```

As of this writing, I got node 7.2, npm 4.0 and mongodb 3.2. The javascript ecosystem is a very fast moving one. Some
of the code mentioned hereon might get obsolete in a couple of years but the general idea remains the same.

### Setup the initial package.json
In the node world `package.json` is the file which contains the details of our dependencies and run configurations.
In Rails world, it is analogous to `Gemfile` but on steroids. In java world it is kind of analogous to `build.gradle`
or a `pom.xml`.

```sh
mkdir -p ~/Documents/Projects/js-snipcode
cd !$
npm init
```
 The second command might seem unfamiliar. !$ represents the last paramter of previous command in bash and zsh. So it
will just take you to the newly created directory from the previous step.

The `npm init` is interactive and it will first ask you for a project name. Then configure the version and description.
Next it asks for the entry point for the app. You can leave it as `index.js`. Keep pressing enter to accept the
defaults for other fields or change if you like. For now we ignore the test command. A `package.json` file will be
created in the end.

### Install Express
Express is the web framework library that we are going to use. Now let's add a dependency on Express, add babel
transpilation of ES6 and get from zero to &quot;Hello World!&quot;.

```sh
npm install express --save
npm install --save-dev babel-cli babel-preset-latest
```

Running the above commands will download Express, babel-cli and babel's latest preset. `--save` flag makes an entry in
the `dependencies` section of package.json. Guess what does the `--save-dev` flag do? You got it right! It makes an
entry in `devDependencies` in `package.json`. Dev dependencies are different from dependencies in that they
are not required for running the app. They are only used during development, for instance for transpilation during
build. Let's also add two target scripts named `build` and `start` in the `package.json`. Running `npm run build` will
tell babel to transpile everything in `src` directory recursively and output the transpiled files in `dist` directory.
`npm run start` in the root directory of our project will start our web server by running the command
`node dist/index.js`. Also, don't forget `mkdir src; mv index.js src`. Here's the updated `package.json`.

```json
{
  &quot;name&quot;: &quot;js-snipcode&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;,
  &quot;description&quot;: &quot;API in NodeJS for the Snipcode project.&quot;,
  &quot;main&quot;: &quot;index.js&quot;,
  &quot;scripts&quot;: {
    &quot;test&quot;: &quot;echo \&quot;Error: no test specified\&quot; &amp;&amp; exit 1&quot;,
    &quot;build&quot;: &quot;babel src -d dist --presets latest&quot;,
    &quot;start&quot;: &quot;node dist/index.js&quot;
  },
  &quot;author&quot;: &quot;Prabhakar Kumar&quot;,
  &quot;license&quot;: &quot;ISC&quot;,
  &quot;dependencies&quot;: {
    &quot;express&quot;: &quot;^4.14.0&quot;
  },
  &quot;devDependencies&quot;: {
    &quot;babel-cli&quot;: &quot;^6.18.0&quot;,
    &quot;babel-preset-latest&quot;: &quot;^6.18.0&quot;
  }
}
```

### Zero to Hello World!
Let's edit our `src/index.js` and write relevant code to make a Hello World app.

```javascript
import express from 'express';

const app = express();

app.get('/', (req, res) =&gt;
  res.send('Hello World!')
);

app.listen(3000, () =&gt; 
  console.log('Server is up on port 3000!')
);
```

Now let's build and run.

```sh
npm run build
npm run start
```

If everything went well as we discussed, you will see a message on the console that says server is up on port 3000.
Now if you fire up firefox or chromium and go to [localhost:3000](http://localhost:3000), you will see the evergreen
'Hello world!' message staring at you on a white screen.

Before we start writing more code, I'll explain the meaning of the lines in our Hello World web app. But, before that
let's pivot and focus on learning our database and design a basic data model.

### Play with mongo console
Let's play with mongo console a bit to gain some familiarity with it.

```text
$ mongo
MongoDB shell version: 3.2.10
connecting to: test
Welcome to the MongoDB shell.
For interactive help, type &quot;help&quot;.
For more comprehensive documentation, see
        http://docs.mongodb.org/
Questions? Try the support group
        http://groups.google.com/group/mongodb-user
Server has startup warnings:
2016-12-04T07:40:24.545-0500 I CONTROL  [initandlisten]
2016-12-04T07:40:24.545-0500 I CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/enabled is 'always'.
2016-12-04T07:40:24.545-0500 I CONTROL  [initandlisten] **        We suggest setting it to 'never'
2016-12-04T07:40:24.545-0500 I CONTROL  [initandlisten]
&gt; show databases
local  0.000GB
&gt; use snipcode
switched to db snipcode
&gt; db.users.insert({name: &quot;John Doe&quot;, country: &quot;United States&quot;, &quot;sex&quot;: 'M', dob: ISODate('1970-01-01')})
WriteResult({ &quot;nInserted&quot; : 1 })
&gt; db.users.find()
{ &quot;_id&quot; : ObjectId(&quot;58441a27d01af6e5b1ae6f74&quot;), &quot;name&quot; : &quot;John Doe&quot;, &quot;country&quot; : &quot;United States&quot;, &quot;sex&quot; : &quot;M&quot;, &quot;dob&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;) ers.insert({name: &quot;Jane Doe&quot;, country: &quot;United States&quot;, &quot;sex&quot;: 'F', dob: ISODate('1970-01-01')})
WriteResult({ &quot;nInserted&quot; : 1 })
&gt; db.users.find()
{ &quot;_id&quot; : ObjectId(&quot;58441a27d01af6e5b1ae6f74&quot;), &quot;name&quot; : &quot;John Doe&quot;, &quot;country&quot; : &quot;United States&quot;, &quot;sex&quot; : &quot;M&quot;, &quot;dob&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;) }
{ &quot;_id&quot; : ObjectId(&quot;58441a5bd01af6e5b1ae6f75&quot;), &quot;name&quot; : &quot;Jane Doe&quot;, &quot;country&quot; : &quot;United States&quot;, &quot;sex&quot; : &quot;F&quot;, &quot;dob&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;) }
&gt; db.users.find({sex: 'F'})
{ &quot;_id&quot; : ObjectId(&quot;58441a5bd01af6e5b1ae6f75&quot;), &quot;name&quot; : &quot;Jane Doe&quot;, &quot;country&quot; : &quot;United States&quot;, &quot;sex&quot; : &quot;F&quot;, &quot;dob&quot; : ISODate(&quot;1970-01-01T00:00:00Z&quot;) }
```

Similar to MySQL and friends (MariaDB, Aurora), Mongodb organizes it's data in databases. We don't need to explicitly
create a database unlike MySQL. It gets created as soon as we try to write something to it. Inside a Mongodb database,
the database is organized in collections. A collection is analogous to tables in RDBMS, and more flexible. A
collection doesn't have a fixed schema. So, one document (record/row in RDBMS) can have fields different from others
in the same collection. In the above example, we switched to database named `snipcode`. Then we inserted a document
describing a user named 'John Doe'. Upon doing a `find()` (which is analogous to SQL's SELECT) on that collection we
got the record, along with a unique record ID called ObjectID. Then we inserted another record for `Jane Doe`. Then we
ran a query which returns all female users. For this, we provided the query condition `sex: 'F'` which is similar to
WHERE sex = 'F' in SQL.

As of now, this much familiarity is enough for us to get started. We will dive deeper as and when we need.


## Next part

In this part we got bootstrapped and got our hands dirty. In the next part we will start with data modeling and
continue further. Stay tuned for it.</content><author><name>Prabhakar Kumar</name></author><category term="linux" /><category term="nodejs" /><category term="mongodb" /><category term="es6" /><summary type="html">Introduction NodeJS has been gaining a lot of traction recently, especially for web applications. Lots of new web applications are now being built with tools from the Node ecosystem. A possible reason is the umpteen number of javascript developers out in the wild. Other one is the blazing speed of V8 engine. Heck, even shiny desktop apps are now being built with Node viz. Popcorn Time.</summary></entry><entry><title type="html">Elegant solutions to common interview problems - Part 1 - Linked Lists</title><link href="http://localhost:4000/2017/03/10/elegant-solutions-to-common-interview-problems.html" rel="alternate" type="text/html" title="Elegant solutions to common interview problems - Part 1 - Linked Lists" /><published>2017-03-10T10:48:24-08:00</published><updated>2017-03-10T10:48:24-08:00</updated><id>http://localhost:4000/2017/03/10/elegant-solutions-to-common-interview-problems</id><content type="html" xml:base="http://localhost:4000/2017/03/10/elegant-solutions-to-common-interview-problems.html">&gt; Define a linked list node for integers

{% highlight cpp %}
class Node {
    public:
    int data;
    Node* next;
};
{% endhighlight %}

&gt; Reverse a singly linked list in-place iteratively

{% highlight cpp %}
Node* reverseIterative(Node* head) {
    // Initialize two pointers, current(ptr) and previous
    Node* ptr = head;
    Node* prev = NULL;
    // At each node, we point it's next to previous
    while (ptr) {
        Node* next = ptr-&gt;next;
        ptr-&gt;next = prev;
        // Advance both pointers
        prev = ptr;
        ptr = next;
    }
    // Point head to last node
    head = prev;
    return head;
}
{% endhighlight %}

&gt; Reverse a singly linked list in-place recursively

{% highlight cpp %}
Node* reverseRecursive(Node* head) {
    if (!head || !head-&gt;next) {
        return head;
    }
    Node* rest = reverseRecursive(head-&gt;next);
    // Point head's next node towards head
    head-&gt;next-&gt;next = head;
    head-&gt;next = NULL;
    return rest;
}
{% endhighlight %}

&gt; Traverse a linked list recursively

{% highlight cpp %}
void traverse(Node* head) {
    if (!head) {
        return;
    } else {
        cout&lt;&lt;head-&gt;data&lt;&lt;endl;
    }
    traverse(head-&gt;next);
}
{% endhighlight %}

&gt; Traverse a linked list recursively in reverse order

{% highlight cpp %}
void traverseReverse(Node* head) {
    if (!head) {
        return;
    }
    traverseReverse(head-&gt;next);
    cout&lt;&lt;head-&gt;data&lt;&lt;endl;
}
{% endhighlight %}

&gt; Print nth element of linked list (counted from 1)

{% highlight cpp %}
void printNthElement(Node* head, int n) {
    if (!head) {
        return;
    }
    if (n == 1) {
        cout&lt;&lt;head-&gt;data;
    }
    printNthElement(head-&gt;next, n-1);
}
{% endhighlight %}

&gt; Print nth element from last in linked list 

{% highlight cpp %}
void printNthLastElement(Node* head, int* n) {
    if (!head) {
        return;
    }
    printNthLastElement(head-&gt;next, n);
    *n = *n - 1;
    if (*n == 0) {
        cout&lt;&lt;head-&gt;data;
        return;
    }
}
{% endhighlight %}</content><author><name>Prabhakar Kumar</name></author><category term="computer-science" /><summary type="html">Define a linked list node for integers</summary></entry><entry><title type="html">Compile goaccess from source on CentOS 7</title><link href="http://localhost:4000/2016/07/07/compile-goaccess-from-source-on-centos-7.html" rel="alternate" type="text/html" title="Compile goaccess from source on CentOS 7" /><published>2016-07-07T19:00:32-07:00</published><updated>2016-07-07T19:00:32-07:00</updated><id>http://localhost:4000/2016/07/07/compile-goaccess-from-source-on-centos-7</id><content type="html" xml:base="http://localhost:4000/2016/07/07/compile-goaccess-from-source-on-centos-7.html">[Goaccess](https://goaccess.io) is a neat little utility which scans through your web server (I use nginx) logs and generates a nice HTML report of 
your site's access statistics. Here is a [sample from their site](http://rt.goaccess.io). I run CentOS 7 on an EC2 Instance where I run a pet project. I wanted a light web analyzer that just works. I have
[epel](https://fedoraproject.org/wiki/EPEL) enabled on my instance so I just installed goaccess by `sudo yum install goaccess`. It worked and I was able
to see some stats. But then I figured out that the version from repositories is quite an old one at 0.9.8. The latest one is 1.0.2 from the website.
So, I just uninstalled it `sudo yum remove goaccess`.

I downloaded the source tarball and after a few hiccups I was able to get it running. There are two quirks you need to watch out for. First one is
that, enabling geoip in goaccess requires installation of maxmind's geoip database. The installed database from yum is very small, so you need to
update it and change symlink /usr/share/GeoIP/GeoIP.dat to point to /usr/share/GeoIP/GeoLiteCountry.dat. Second quirk is that by default the configure
script doesn't figure out the geoip devel libs path so you need to manually point to it while configuring.

```sh
sudo yum install GeoIP
geoipupdate
sudo rm /usr/share/GeoIP.dat
sudo ln -s /usr/share/{GeoLiteCountry,GeoIP}.dat 

wget http://tar.goaccess.io/goaccess-1.0.2.tar.gz
tar -xzvf goaccess-1.0.2.tar.gz
cd goaccess-1.0.2
LD_FLAGS='-L/usr/lib64/' ./configure --enable-utf8 --enable-geoip
make
sudo make install
```

I also created a daily crontab entry to generate reports daily and put it in my `public` folder of rails directory, all the files in which I serve
statically from nginx. So I can just type http://mydomain.com/access.html and see the reports. Here is my crontab entry.

```sh crontab
@daily /bin/zcat -f /var/log/nginx/access.log* | /usr/local/bin/goaccess -a -o /home/ec2-user/my_rails_dir/public/access.html
```</content><author><name>Prabhakar Kumar</name></author><category term="linux" /><category term="web-apps" /><summary type="html">Goaccess is a neat little utility which scans through your web server (I use nginx) logs and generates a nice HTML report of your site’s access statistics. Here is a sample from their site. I run CentOS 7 on an EC2 Instance where I run a pet project. I wanted a light web analyzer that just works. I have epel enabled on my instance so I just installed goaccess by sudo yum install goaccess. It worked and I was able to see some stats. But then I figured out that the version from repositories is quite an old one at 0.9.8. The latest one is 1.0.2 from the website. So, I just uninstalled it sudo yum remove goaccess.</summary></entry><entry><title type="html">Configure DatabaseCleaner with Rspec for Mongoid 5 and beyond</title><link href="http://localhost:4000/2016/06/26/configure-database-cleaner-with-rspec-for-mongoid-5-and-beyond.html" rel="alternate" type="text/html" title="Configure DatabaseCleaner with Rspec for Mongoid 5 and beyond" /><published>2016-06-26T18:05:22-07:00</published><updated>2016-06-26T18:05:22-07:00</updated><id>http://localhost:4000/2016/06/26/configure-database-cleaner-with-rspec-for-mongoid-5-and-beyond</id><content type="html" xml:base="http://localhost:4000/2016/06/26/configure-database-cleaner-with-rspec-for-mongoid-5-and-beyond.html">[Database Cleaner](https://github.com/DatabaseCleaner/database_cleaner) is a nifty gem for streamlining tests. Configuring it is straightforward
but it didn't work for me out of the box for Mongoid 5.

Here's my Gemfile segment for testing.

```ruby
group :test do
  gem &quot;factory_girl_rails&quot;
  gem &quot;rspec-rails&quot;, '~&gt; 3.4'
  gem 'faker'
  gem 'database_cleaner', git: 'git://github.com/DatabaseCleaner/database_cleaner.git'
end
```

I added a `require 'support/database_cleaner'` in my `spec_helper`. And here's my `database_cleaner.rb`.

```ruby
RSpec.configure do |config|

  config.before(:suite) do
    DatabaseCleaner.strategy = :truncation
    DatabaseCleaner.clean
  end

  config.before(:each) do
    DatabaseCleaner.start
  end

  config.after(:each) do
    DatabaseCleaner.clean
  end

end
```

When I ran my spec, I was greeted with an unexpected ugly error.

```text
/home/prabhakar/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/bundler/gems/database_cleaner-f052d64d3be9/lib/database_cleaner/mongo2/truncation_mixin.rb:29:in `collections': undefined method `collections' for #&lt;Mongo::Client:0x47021052998580 cluster=127.0.0.1:27017&gt; (NoMethodError)
	from /home/prabhakar/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/bundler/gems/database_cleaner-f052d64d3be9/lib/database_cleaner/mongo2/truncation_mixin.rb:9:in `clean'
	from /home/prabhakar/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/bundler/gems/database_cleaner-f052d64d3be9/lib/database_cleaner/base.rb:92:in `clean'
	from /home/prabhakar/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/bundler/gems/database_cleaner-f052d64d3be9/lib/database_cleaner/configuration.rb:79:in `block in clean'
	from /home/prabhakar/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/bundler/gems/database_cleaner-f052d64d3be9/lib/database_cleaner/configuration.rb:79:in `each'
	from /home/prabhakar/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/bundler/gems/database_cleaner-f052d64d3be9/lib/database_cleaner/configuration.rb:79:in `clean'
```

After looking at the code in the gem, I finally found out what was wrong in it. I created a
[commit for fix](https://github.com/prabhakar97/database_cleaner/commit/3cf0d1b81e4a118fd173d697f032a9aff4f431de) and submitted a pull request to
the maintainer. Looking at my commit and the relevant files [truncation.rb](https://github.com/DatabaseCleaner/database_cleaner/blob/master/lib/database_cleaner/mongoid/truncation.rb) and [truncation_mixin.rb](https://github.com/DatabaseCleaner/database_cleaner/blob/master/lib/database_cleaner/mongo2/truncation_mixin.rb), you should be able to piece together the problem.

Meanwhile, if you are using Mongoid 5 and aren't able to get it working you could simply point to my fork of database_cleaner by updating your Gemfile
to have: 

```ruby
gem 'database_cleaner', git: 'git://github.com/prabhakar97/database_cleaner.git'
```</content><author><name>Prabhakar Kumar</name></author><category term="rails" /><category term="ruby" /><category term="mongodb" /><summary type="html">Database Cleaner is a nifty gem for streamlining tests. Configuring it is straightforward but it didn’t work for me out of the box for Mongoid 5.</summary></entry><entry><title type="html">Starting with Yesod on Arch Linux</title><link href="http://localhost:4000/2016/06/23/starting-with-yesod-on-arch-linux.html" rel="alternate" type="text/html" title="Starting with Yesod on Arch Linux" /><published>2016-06-23T16:59:39-07:00</published><updated>2016-06-23T16:59:39-07:00</updated><id>http://localhost:4000/2016/06/23/starting-with-yesod-on-arch-linux</id><content type="html" xml:base="http://localhost:4000/2016/06/23/starting-with-yesod-on-arch-linux.html">As part of my eagerness to learn Haskell, I thought it might be a good idea to start building real world applications in Haskell. I chose
[Yesod](http://www.yesodweb.com/) to start building a dynamic web application backed by as database. However, the getting started page on
Yesod website didn't exactly work as advertised, probably because the [Haskell Stack](http://docs.haskellstack.org/en/stable/README/)
had some API change. These steps worked for me.

* Install haskell-stack. It lets you develop, build and test Haskell apps without creating dependency version issues across projects.
If you are coming from a ruby world, it is similar to rbenv.


`sudo pacman -S haskell-stack`


* Checkout the *yesod-mongodb* template. This is going to create a `haskell-webapp` directory and checkout stuff from the `yesod-mongo` template.
Make sure that you don't provide a directory name which is the name of a package, like yesod, ghci etc.

`stack new haskell-webapp yesod-mongo`

* Install [libtinfo from AUR](https://aur.archlinux.org/packages/libtinfo/) because Arch screwed up (from stack perspective). There is
another nuance to take care of. Edit the *PKGBUILD* and ensure that the line that simlinks `/usr/lib/libtinfo.so.5` is uncommented. Otherwise
the next step doesn't work.

* Setup GHCi and friends.

`stack build yesod-bin cabal-install --install-ghc`

* Build the libs.

`stack build`

* Hello World!

`stack exec -- yesod devel`

To access your server visit [http://localhost:3000](http://localhost:3000)</content><author><name>Prabhakar Kumar</name></author><category term="haskell" /><category term="web-apps" /><summary type="html">As part of my eagerness to learn Haskell, I thought it might be a good idea to start building real world applications in Haskell. I chose Yesod to start building a dynamic web application backed by as database. However, the getting started page on Yesod website didn’t exactly work as advertised, probably because the Haskell Stack had some API change. These steps worked for me.</summary></entry></feed>